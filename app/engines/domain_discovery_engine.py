"""
å¾ªç¯åŸŸåå‘ç°å’Œçˆ¬å–å¼•æ“
æŒç»­ä»åŸŸååº“å‘ç°æ–°åŸŸåï¼Œå¾ªç¯çˆ¬å–ç›´åˆ°æ‰¾ä¸åˆ°æ–°åŸŸåä¸ºæ­¢
"""

import asyncio
import time
from typing import Dict, List, Set, Optional, Any
from datetime import datetime
from urllib.parse import urlparse, urljoin
from dataclasses import dataclass, field
import tldextract

from app.core.logging import TaskLogger
from app.core.database import AsyncSessionLocal
from app.models.domain import DomainRecord, DomainCategory, DomainStatus, DiscoveryMethod, RiskLevel
from app.engines.link_crawler import LinkCrawlerEngine
from sqlalchemy import select, update, and_, or_, func


@dataclass 
class DiscoveryRound:
    """å‘ç°è½®æ¬¡ä¿¡æ¯"""
    round_number: int
    start_time: datetime
    end_time: Optional[datetime] = None
    domains_to_crawl: int = 0
    new_domains_found: int = 0
    pages_crawled: int = 0
    total_links_found: int = 0
    errors: List[str] = field(default_factory=list)
    



class ContinuousDomainDiscoveryEngine:
    """æŒç»­åŸŸåå‘ç°å¼•æ“"""
    
    def __init__(self, task_id: str, user_id: str, target_domain: str):
        self.task_id = task_id
        self.user_id = user_id
        self.target_domain = target_domain.lower().strip()
        self.logger = TaskLogger(task_id, user_id)
        
        # çˆ¬å–å¼•æ“
        self.crawler_engine = LinkCrawlerEngine(task_id, user_id)
        
        # è¿è¡ŒçŠ¶æ€
        self.is_running = False
        self.is_cancelled = False
        self.current_round = 0
        self.total_domains_discovered = 0
        self.total_pages_crawled = 0
        
        # å‘ç°è½®æ¬¡è®°å½•
        self.discovery_rounds: List[DiscoveryRound] = []
        
        # ç›®æ ‡åŸŸåçš„æ³¨å†ŒåŸŸåä¿¡æ¯
        self.target_parts = tldextract.extract(self.target_domain)
        self.target_registered_domain = self.target_parts.registered_domain.lower()
        
        self.logger.info(f"åˆå§‹åŒ–æŒç»­åŸŸåå‘ç°å¼•æ“: ç›®æ ‡åŸŸå={self.target_domain}")
    
    async def start_continuous_discovery(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """å¼€å§‹æŒç»­åŸŸåå‘ç°"""
        self.logger.info("ğŸš€ å¼€å§‹æŒç»­åŸŸåå‘ç°å’Œçˆ¬å–...")
        
        self.is_running = True
        self.is_cancelled = False
        
        try:
            # 1. åˆå§‹åŒ–ï¼šå°†ç›®æ ‡åŸŸåæ·»åŠ åˆ°åŸŸååº“
            await self._initialize_target_domain()
            
            # 2. æŒç»­å¾ªç¯å‘ç°å’Œçˆ¬å–
            while not self.is_cancelled:
                discovery_result = await self._execute_discovery_round(config)
                
                # å¦‚æœæ²¡æœ‰å‘ç°æ–°åŸŸåï¼Œç»“æŸå¾ªç¯
                if discovery_result['new_domains_found'] == 0:
                    self.logger.info("âœ… æœªå‘ç°æ–°åŸŸåï¼ŒæŒç»­å‘ç°å®Œæˆ")
                    break
                
                # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è½®æ¬¡é™åˆ¶
                max_rounds = config.get('max_discovery_rounds', 20)
                if self.current_round >= max_rounds:
                    self.logger.warning(f"âš ï¸ è¾¾åˆ°æœ€å¤§å‘ç°è½®æ¬¡é™åˆ¶ ({max_rounds})ï¼Œåœæ­¢å‘ç°")
                    break
                
                # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡åº¦è¯·æ±‚
                await asyncio.sleep(1)
            
            # 3. ç»Ÿè®¡æœ€ç»ˆç»“æœ
            final_stats = await self._calculate_final_statistics()
            
            self.logger.info("ğŸ‰ æŒç»­åŸŸåå‘ç°å®Œæˆ")
            self.logger.info(f"ğŸ“Š æœ€ç»ˆç»Ÿè®¡: æ€»è½®æ¬¡={self.current_round}, å‘ç°åŸŸå={self.total_domains_discovered}, çˆ¬å–é¡µé¢={self.total_pages_crawled}")
            
            return final_stats
            
        except Exception as e:
            self.logger.error(f"æŒç»­åŸŸåå‘ç°å¤±è´¥: {e}")
            raise
        finally:
            self.is_running = False
    
    async def _initialize_target_domain(self):
        """åˆå§‹åŒ–ï¼šå°†ç›®æ ‡åŸŸåæ·»åŠ åˆ°åŸŸååº“"""
        try:
            async with AsyncSessionLocal() as db:
                # æ£€æŸ¥ç›®æ ‡åŸŸåæ˜¯å¦å·²å­˜åœ¨
                existing_query = select(DomainRecord).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.domain == self.target_domain
                    )
                )
                result = await db.execute(existing_query)
                existing_domain = result.scalar_one_or_none()
                
                if not existing_domain:
                    # åˆ›å»ºç›®æ ‡åŸŸåè®°å½•
                    target_record = DomainRecord(
                        task_id=self.task_id,
                        domain=self.target_domain,
                        category=DomainCategory.TARGET_MAIN,
                        status=DomainStatus.DISCOVERED,
                        discovery_method=DiscoveryMethod.MANUAL,
                        is_accessible=True,
                        depth_level=0,
                        risk_level=RiskLevel.SAFE,
                        confidence_score=1.0,
                        tags=["target_domain", "initial"],
                        extra_data={"initialization": True}
                    )
                    
                    db.add(target_record)
                    await db.commit()
                    
                    self.logger.info(f"âœ… ç›®æ ‡åŸŸåå·²æ·»åŠ åˆ°åŸŸååº“: {self.target_domain}")
                else:
                    self.logger.info(f"â„¹ï¸ ç›®æ ‡åŸŸåå·²å­˜åœ¨äºåŸŸååº“: {self.target_domain}")
                    
        except Exception as e:
            self.logger.error(f"åˆå§‹åŒ–ç›®æ ‡åŸŸåå¤±è´¥: {e}")
            raise
    
    async def _execute_discovery_round(self, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œä¸€è½®å‘ç°"""
        self.current_round += 1
        round_info = DiscoveryRound(
            round_number=self.current_round,
            start_time=datetime.utcnow()
        )
        
        self.logger.info(f"ğŸ”„ å¼€å§‹ç¬¬ {self.current_round} è½®åŸŸåå‘ç°...")
        
        try:
            # 1. ä»åŸŸååº“è·å–éœ€è¦çˆ¬å–çš„åŸŸå
            domains_to_crawl = await self._get_domains_to_crawl()
            round_info.domains_to_crawl = len(domains_to_crawl)
            
            if not domains_to_crawl:
                self.logger.info("ğŸ“‹ æ²¡æœ‰éœ€è¦çˆ¬å–çš„åŸŸå")
                round_info.end_time = datetime.utcnow()
                self.discovery_rounds.append(round_info)
                return {'new_domains_found': 0, 'round_info': round_info}
            
            self.logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(domains_to_crawl)} ä¸ªéœ€è¦çˆ¬å–çš„åŸŸå")
            
            # 2. çˆ¬å–è¿™äº›åŸŸå
            crawl_results = await self._crawl_domains(domains_to_crawl, config)
            round_info.pages_crawled = len(crawl_results)
            
            # 3. ä»çˆ¬å–ç»“æœä¸­æå–æ–°åŸŸå
            new_domains = await self._extract_and_save_new_domains(crawl_results)
            round_info.new_domains_found = len(new_domains)
            round_info.total_links_found = sum(len(result.links) for result in crawl_results)
            
            # 4. æ›´æ–°å·²çˆ¬å–åŸŸåçš„çŠ¶æ€
            await self._update_crawled_domains_status(domains_to_crawl)
            
            # 5. å‘é€å®æ—¶æ›´æ–°
            await self._send_real_time_update(round_info, new_domains)
            
            round_info.end_time = datetime.utcnow()
            self.discovery_rounds.append(round_info)
            
            self.total_domains_discovered += round_info.new_domains_found
            self.total_pages_crawled += round_info.pages_crawled
            
            self.logger.info(f"âœ… ç¬¬ {self.current_round} è½®å®Œæˆ: çˆ¬å–åŸŸå={round_info.domains_to_crawl}, å‘ç°æ–°åŸŸå={round_info.new_domains_found}")
            
            return {'new_domains_found': round_info.new_domains_found, 'round_info': round_info}
            
        except Exception as e:
            error_msg = f"ç¬¬ {self.current_round} è½®å‘ç°å¤±è´¥: {e}"
            round_info.errors.append(error_msg)
            round_info.end_time = datetime.utcnow()
            self.discovery_rounds.append(round_info)
            self.logger.error(error_msg)
            raise
    
    async def _get_domains_to_crawl(self) -> List[DomainRecord]:
        """è·å–éœ€è¦çˆ¬å–çš„åŸŸå"""
        try:
            async with AsyncSessionLocal() as db:
                # æŸ¥è¯¢éœ€è¦çˆ¬å–çš„åŸŸåï¼šå·²å‘ç°ä½†æœªçˆ¬å–çš„åŸŸå
                query = select(DomainRecord).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.status.in_([DomainStatus.DISCOVERED, DomainStatus.ACCESSIBLE]),
                        DomainRecord.is_accessible == True,
                        # è¿˜æ²¡æœ‰è¢«çˆ¬å–è¿‡çš„åŸŸåï¼ˆå¯ä»¥é€šè¿‡é¢å¤–å­—æ®µæˆ–çŠ¶æ€åˆ¤æ–­ï¼‰
                        or_(
                            DomainRecord.extra_data == None,
                            ~DomainRecord.extra_data.has_key('crawled'),
                            DomainRecord.extra_data['crawled'].as_boolean() == False
                        )
                    )
                ).limit(20)  # æ¯è½®é™åˆ¶çˆ¬å–æ•°é‡
                
                result = await db.execute(query)
                domains = result.scalars().all()
                
                return list(domains)
                
        except Exception as e:
            self.logger.error(f"è·å–å¾…çˆ¬å–åŸŸåå¤±è´¥: {e}")
            return []
    
    async def _crawl_domains(self, domains: List[DomainRecord], config: Dict[str, Any]) -> List[Any]:
        """çˆ¬å–æŒ‡å®šçš„åŸŸååˆ—è¡¨"""
        all_crawl_results = []
        
        for domain_record in domains:
            try:
                # è·å–åŸŸåå­—ç¬¦ä¸²
                domain_str = str(domain_record.domain)
                
                # æ„å»ºè¦çˆ¬å–çš„URL
                urls_to_crawl = [
                    f"https://{domain_str}",
                    f"http://{domain_str}"
                ]
                
                self.logger.debug(f"ğŸ•·ï¸ å¼€å§‹çˆ¬å–åŸŸå: {domain_str}")
                
                # æ‰§è¡Œçˆ¬å–
                crawl_results = await self.crawler_engine.crawl_domain(
                    domain_str, 
                    urls_to_crawl, 
                    config
                )
                
                all_crawl_results.extend(crawl_results)
                
                self.logger.debug(f"âœ… åŸŸåçˆ¬å–å®Œæˆ: {domain_str}, é¡µé¢æ•°={len(crawl_results)}")
                
                # çŸ­æš‚ä¼‘æ¯é¿å…è¿‡åº¦è¯·æ±‚
                await asyncio.sleep(0.2)
                
            except Exception as e:
                domain_str = str(domain_record.domain)
                self.logger.warning(f"çˆ¬å–åŸŸåå¤±è´¥ {domain_str}: {e}")
                continue
        
        return all_crawl_results
    
    async def _extract_and_save_new_domains(self, crawl_results: List[Any]) -> List[str]:
        """ä»çˆ¬å–ç»“æœä¸­æå–å¹¶ä¿å­˜æ–°åŸŸå"""
        try:
            new_domains = set()
            
            # ä»æ‰€æœ‰çˆ¬å–ç»“æœä¸­æå–é“¾æ¥
            for result in crawl_results:
                links = []
                if hasattr(result, 'links'):
                    links.extend(result.links)
                if hasattr(result, 'resources'):
                    links.extend(result.resources)
                if hasattr(result, 'forms'):
                    links.extend(result.forms)
                
                # ä»é“¾æ¥ä¸­æå–åŸŸå
                for link in links:
                    domain = self._extract_domain_from_url(link)
                    if domain and self._is_valid_domain(domain):
                        new_domains.add(domain)
            
            # è¿‡æ»¤æ‰å·²å­˜åœ¨çš„åŸŸå
            existing_domains = await self._get_existing_domains()
            truly_new_domains = new_domains - existing_domains
            
            if truly_new_domains:
                self.logger.info(f"ğŸ†• å‘ç° {len(truly_new_domains)} ä¸ªæ–°åŸŸå")
                
                # æ‰¹é‡ä¿å­˜æ–°åŸŸååˆ°æ•°æ®åº“
                await self._save_new_domains_to_db(list(truly_new_domains))
            
            return list(truly_new_domains)
            
        except Exception as e:
            self.logger.error(f"æå–å’Œä¿å­˜æ–°åŸŸåå¤±è´¥: {e}")
            return []
    
    def _extract_domain_from_url(self, url: str) -> Optional[str]:
        """ä»URLä¸­æå–åŸŸå"""
        try:
            parsed = urlparse(url)
            if parsed.netloc:
                domain = parsed.netloc.lower()
                # ç§»é™¤ç«¯å£å·
                if ':' in domain:
                    domain = domain.split(':')[0]
                return domain
        except Exception:
            pass
        return None
    
    def _is_valid_domain(self, domain: str) -> bool:
        """æ£€æŸ¥åŸŸåæ˜¯å¦æœ‰æ•ˆ"""
        if not domain or len(domain) < 3:
            return False
        
        # æ’é™¤æ˜æ˜¾çš„æ— æ•ˆåŸŸå
        invalid_patterns = [
            'localhost', '127.0.0.1', '0.0.0.0',
            'example.com', 'test.com', 'demo.com'
        ]
        
        if domain in invalid_patterns:
            return False
        
        # å¿…é¡»åŒ…å«ç‚¹å·
        if '.' not in domain:
            return False
        
        return True
    
    async def _get_existing_domains(self) -> Set[str]:
        """è·å–å·²å­˜åœ¨çš„åŸŸåé›†åˆ"""
        try:
            async with AsyncSessionLocal() as db:
                query = select(DomainRecord.domain).where(
                    DomainRecord.task_id == self.task_id
                )
                result = await db.execute(query)
                domains = result.scalars().all()
                return set(domains)
        except Exception as e:
            self.logger.error(f"è·å–å·²å­˜åœ¨åŸŸåå¤±è´¥: {e}")
            return set()
    
    async def _save_new_domains_to_db(self, domains: List[str]):
        """æ‰¹é‡ä¿å­˜æ–°åŸŸååˆ°æ•°æ®åº“"""
        try:
            async with AsyncSessionLocal() as db:
                domain_records = []
                
                for domain in domains:
                    # åˆ¤æ–­åŸŸåç±»å‹
                    category = self._classify_domain(domain)
                    
                    record = DomainRecord(
                        task_id=self.task_id,
                        domain=domain,
                        category=category,
                        status=DomainStatus.DISCOVERED,
                        discovery_method=DiscoveryMethod.LINK_CRAWLING,
                        is_accessible=True,  # é»˜è®¤è®¾ä¸ºå¯è®¿é—®ï¼Œåç»­éªŒè¯
                        depth_level=self.current_round,
                        risk_level=RiskLevel.LOW if category != DomainCategory.THIRD_PARTY else RiskLevel.MEDIUM,
                        confidence_score=0.8,
                        tags=["auto_discovered", f"round_{self.current_round}"],
                        extra_data={
                            "discovered_in_round": self.current_round,
                            "crawled": False
                        }
                    )
                    
                    domain_records.append(record)
                
                # æ‰¹é‡æ·»åŠ 
                db.add_all(domain_records)
                await db.commit()
                
                self.logger.info(f"âœ… å·²ä¿å­˜ {len(domain_records)} ä¸ªæ–°åŸŸååˆ°æ•°æ®åº“")
                
        except Exception as e:
            self.logger.error(f"ä¿å­˜æ–°åŸŸååˆ°æ•°æ®åº“å¤±è´¥: {e}")
            raise
    
    def _classify_domain(self, domain: str) -> str:
        """ç®€å•çš„åŸŸååˆ†ç±»"""
        try:
            # ä½¿ç”¨tldextractè§£æåŸŸå
            domain_parts = tldextract.extract(domain)
            domain_registered = domain_parts.registered_domain.lower()
            
            # å¦‚æœæ³¨å†ŒåŸŸåç›¸åŒï¼Œåˆ™æ˜¯ç›®æ ‡ç›¸å…³åŸŸå
            if domain_registered == self.target_registered_domain:
                if domain == self.target_domain:
                    return DomainCategory.TARGET_MAIN
                else:
                    return DomainCategory.TARGET_SUBDOMAIN
            else:
                return DomainCategory.THIRD_PARTY
                
        except Exception:
            return DomainCategory.UNKNOWN
    
    async def _update_crawled_domains_status(self, domains: List[DomainRecord]):
        """æ›´æ–°å·²çˆ¬å–åŸŸåçš„çŠ¶æ€"""
        try:
            async with AsyncSessionLocal() as db:
                for domain_record in domains:
                    # è·å–ç°æœ‰çš„extra_dataï¼Œç¡®ä¿æ˜¯å­—å…¸ç±»å‹
                    existing_extra_data = domain_record.extra_data if isinstance(domain_record.extra_data, dict) else {}
                    
                    # æ›´æ–°åŸŸåçŠ¶æ€ä¸ºå·²çˆ¬å–
                    update_query = update(DomainRecord).where(
                        DomainRecord.id == domain_record.id
                    ).values(
                        status=DomainStatus.ACCESSIBLE,
                        last_accessed_at=datetime.utcnow(),
                        extra_data={
                            **existing_extra_data,
                            "crawled": True,
                            "crawled_at": datetime.utcnow().isoformat(),
                            "crawled_in_round": self.current_round
                        }
                    )
                    
                    await db.execute(update_query)
                
                await db.commit()
                
        except Exception as e:
            self.logger.error(f"æ›´æ–°åŸŸåçŠ¶æ€å¤±è´¥: {e}")
    
    async def _send_real_time_update(self, round_info: DiscoveryRound, new_domains: List[str]):
        """å‘é€å®æ—¶æ›´æ–°"""
        try:
            # ä½¿ç”¨WebSocketå‘é€å®æ—¶æ›´æ–°
            from app.websocket.domain_websocket import task_notifier, notify_domain_stats_update
            
            # å‘é€è½®æ¬¡å®Œæˆé€šçŸ¥
            round_data = {
                "round_number": round_info.round_number,
                "domains_crawled": round_info.domains_to_crawl,
                "new_domains_found": round_info.new_domains_found,
                "pages_crawled": round_info.pages_crawled,
                "total_links_found": round_info.total_links_found,
                "duration_seconds": (round_info.end_time - round_info.start_time).total_seconds() if round_info.end_time else 0,
                "errors": round_info.errors
            }
            
            await task_notifier.notify_continuous_discovery_round(self.task_id, round_data)
            
            # å¦‚æœå‘ç°äº†æ–°åŸŸåï¼Œå‘é€åŸŸåå‘ç°é€šçŸ¥
            if new_domains:
                await task_notifier.notify_domain_discovered(
                    self.task_id, 
                    new_domains, 
                    round_info.round_number
                )
            
            # æ›´æ–°åŸŸåç»Ÿè®¡æ•°æ®
            await notify_domain_stats_update(self.task_id)
            
            self.logger.debug(f"ğŸ“¡ å‘é€å®æ—¶æ›´æ–°: è½®æ¬¡{round_info.round_number}, æ–°åŸŸå{len(new_domains)}ä¸ª")
            
        except Exception as e:
            self.logger.warning(f"å‘é€å®æ—¶æ›´æ–°å¤±è´¥: {e}")
    
    async def _calculate_final_statistics(self) -> Dict[str, Any]:
        """è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        try:
            async with AsyncSessionLocal() as db:
                # ç»Ÿè®¡å„ç±»åŸŸåæ•°é‡
                total_query = select(DomainRecord).where(DomainRecord.task_id == self.task_id)
                result = await db.execute(total_query)
                all_domains = result.scalars().all()
                
                # ä½¿ç”¨æ•°æ®åº“æŸ¥è¯¢è®¡ç®—å„ç§ç»Ÿè®¡æ•°æ®
                target_main_query = select(func.count()).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.category == DomainCategory.TARGET_MAIN
                    )
                )
                target_main_result = await db.execute(target_main_query)
                target_main_count = target_main_result.scalar() or 0
                
                target_subdomain_query = select(func.count()).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.category == DomainCategory.TARGET_SUBDOMAIN
                    )
                )
                target_subdomain_result = await db.execute(target_subdomain_query)
                target_subdomain_count = target_subdomain_result.scalar() or 0
                
                third_party_query = select(func.count()).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.category == DomainCategory.THIRD_PARTY
                    )
                )
                third_party_result = await db.execute(third_party_query)
                third_party_count = third_party_result.scalar() or 0
                
                unknown_query = select(func.count()).where(
                    and_(
                        DomainRecord.task_id == self.task_id,
                        DomainRecord.category == DomainCategory.UNKNOWN
                    )
                )
                unknown_result = await db.execute(unknown_query)
                unknown_count = unknown_result.scalar() or 0
                
                stats = {
                    "total_rounds": self.current_round,
                    "total_domains": len(all_domains),
                    "total_pages_crawled": self.total_pages_crawled,
                    "target_main_count": target_main_count,
                    "target_subdomain_count": target_subdomain_count,
                    "third_party_count": third_party_count,
                    "unknown_count": unknown_count,
                    "discovery_rounds": [
                        {
                            "round": r.round_number,
                            "domains_crawled": r.domains_to_crawl,
                            "new_domains_found": r.new_domains_found,
                            "pages_crawled": r.pages_crawled,
                            "duration_seconds": (r.end_time - r.start_time).total_seconds() if r.end_time else 0
                        }
                        for r in self.discovery_rounds
                    ]
                }
                
                return stats
                
        except Exception as e:
            self.logger.error(f"è®¡ç®—æœ€ç»ˆç»Ÿè®¡å¤±è´¥: {e}")
            return {"error": str(e)}
    
    async def cancel_discovery(self):
        """å–æ¶ˆå‘ç°è¿‡ç¨‹"""
        self.is_cancelled = True
        self.logger.info("ğŸ›‘ æ”¶åˆ°å–æ¶ˆå‘ç°è¯·æ±‚")
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            "is_running": self.is_running,
            "is_cancelled": self.is_cancelled,
            "current_round": self.current_round,
            "total_domains_discovered": self.total_domains_discovered,
            "total_pages_crawled": self.total_pages_crawled,
            "discovery_rounds": len(self.discovery_rounds)
        }