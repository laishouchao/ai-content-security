import asyncio
import time
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum

from app.core.logging import TaskLogger
from app.engines.subdomain_discovery import SubdomainDiscoveryEngine, SubdomainResult
from app.engines.link_crawler import LinkCrawlerEngine, CrawlResult
from app.engines.third_party_identifier import ThirdPartyIdentifierEngine, ThirdPartyDomainResult
from app.engines.content_capture import ContentCaptureEngine, ContentResult
from app.engines.ai_analysis import AIAnalysisEngine
from app.models.task import ViolationRecord


class PipelineStage(Enum):
    """æµæ°´çº¿é˜¶æ®µ"""
    DISCOVERY = "discovery"
    CRAWLING = "crawling"
    ANALYSIS = "analysis"


@dataclass
class ScanEvent:
    """æ‰«æäº‹ä»¶"""
    timestamp: float
    stage: PipelineStage
    event_type: str
    data: Dict[str, Any]
    task_id: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp,
            'stage': self.stage.value,
            'event_type': self.event_type,
            'data': self.data,
            'task_id': self.task_id
        }


class EventStore:
    """äº‹ä»¶å­˜å‚¨å™¨"""
    
    def __init__(self, task_id: str):
        self.task_id = task_id
        self.events: List[ScanEvent] = []
        self.subscribers = []
    
    async def emit(self, stage: PipelineStage, event_type: str, data: Dict[str, Any]):
        """å‘å°„äº‹ä»¶"""
        event = ScanEvent(
            timestamp=time.time(),
            stage=stage,
            event_type=event_type,
            data=data,
            task_id=self.task_id
        )
        
        self.events.append(event)
        
        # é€šçŸ¥æ‰€æœ‰è®¢é˜…è€…
        for subscriber in self.subscribers:
            try:
                await subscriber(event)
            except Exception as e:
                print(f"Event subscriber error: {e}")
    
    def subscribe(self, callback):
        """è®¢é˜…äº‹ä»¶"""
        self.subscribers.append(callback)
    
    def get_events(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰äº‹ä»¶"""
        return [event.to_dict() for event in self.events]


class PipelineQueue:
    """è½¨é—´é€šä¿¡é˜Ÿåˆ—"""
    
    def __init__(self, maxsize: int = 1000):
        self.queue = asyncio.Queue(maxsize=maxsize)
        self.processed_count = 0
        self.error_count = 0
    
    async def put(self, item: Any):
        """æ”¾å…¥é˜Ÿåˆ—"""
        await self.queue.put(item)
    
    async def get(self) -> Any:
        """ä»é˜Ÿåˆ—è·å–"""
        item = await self.queue.get()
        self.processed_count += 1
        return item
    
    def qsize(self) -> int:
        """é˜Ÿåˆ—å¤§å°"""
        return self.queue.qsize()
    
    def task_done(self):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ"""
        self.queue.task_done()


class ParallelScanExecutor:
    """å¹¶è¡Œæ‰«ææ‰§è¡Œå™¨"""
    
    def __init__(self, task_id: str, user_id: str):
        self.task_id = task_id
        self.user_id = user_id
        self.logger = TaskLogger(task_id, user_id)
        
        # äº‹ä»¶å­˜å‚¨
        self.event_store = EventStore(task_id)
        
        # å¼•æ“å®ä¾‹
        self.subdomain_engine = SubdomainDiscoveryEngine(task_id, user_id)
        self.crawler_engine = LinkCrawlerEngine(task_id, user_id)
        self.identifier_engine = ThirdPartyIdentifierEngine(task_id, user_id)
        self.capture_engine = ContentCaptureEngine(task_id, user_id)
        self.ai_engine = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # è½¨é—´é˜Ÿåˆ—
        self.discovery_to_crawl = PipelineQueue(maxsize=2000)
        self.crawl_to_analysis = PipelineQueue(maxsize=2000)
        
        # æ‰§è¡ŒçŠ¶æ€
        self.is_running = False
        self.is_cancelled = False
        self.start_time = None
        self.end_time = None
        
        # ç»“æœæ”¶é›†
        self.results = {
            'subdomains': [],
            'crawl_results': [],
            'third_party_domains': [],
            'content_results': [],
            'violation_records': [],
            'statistics': {}
        }
    
    async def execute_scan(self, target_domain: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶è¡Œæ‰«æ"""
        self.is_running = True
        self.is_cancelled = False
        self.start_time = time.time()
        
        try:
            await self.event_store.emit(
                PipelineStage.DISCOVERY, 
                'scan_started', 
                {'target_domain': target_domain, 'config': config}
            )
            
            # å¯åŠ¨ä¸‰è½¨å¹¶è¡Œæµæ°´çº¿
            discovery_task = asyncio.create_task(
                self._discovery_pipeline(target_domain, config)
            )
            crawl_task = asyncio.create_task(
                self._crawling_pipeline(config)
            )
            analysis_task = asyncio.create_task(
                self._analysis_pipeline(config)
            )
            
            # ç­‰å¾…æ‰€æœ‰è½¨å®Œæˆ
            await asyncio.gather(
                discovery_task, 
                crawl_task, 
                analysis_task,
                return_exceptions=True
            )
            
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
            await self._calculate_final_statistics()
            
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'scan_completed',
                {'duration': time.time() - self.start_time}
            )
            
            return self._build_scan_result()
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œæ‰«ææ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'scan_failed',
                {'error': str(e)}
            )
            raise
        finally:
            self.is_running = False
            self.end_time = time.time()
    
    async def _discovery_pipeline(self, target_domain: str, config: Dict[str, Any]):
        """å‘ç°è½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'stage_started',
                {'stage': 'subdomain_discovery'}
            )
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å­åŸŸåå‘ç°æ–¹æ³•
            discovery_tasks = []
            
            # DNSçˆ†ç ´ï¼ˆé«˜å¹¶å‘ï¼‰
            if config.get('subdomain_discovery_enabled', True):
                discovery_tasks.append(
                    self._enhanced_dns_discovery(target_domain, config)
                )
            
            # è¯ä¹¦é€æ˜æ—¥å¿—ï¼ˆå¤šæºï¼‰
            if config.get('certificate_discovery_enabled', True):
                discovery_tasks.append(
                    self._enhanced_certificate_discovery(target_domain, config)
                )
            
            # è¢«åŠ¨DNS
            if config.get('passive_dns_enabled', True):
                discovery_tasks.append(
                    self._passive_dns_discovery(target_domain, config)
                )
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å‘ç°ä»»åŠ¡
            discovery_results = await asyncio.gather(*discovery_tasks, return_exceptions=True)
            
            # åˆå¹¶å»é‡ç»“æœ
            all_subdomains = set()
            for result in discovery_results:
                if isinstance(result, list):
                    all_subdomains.update(result)
                elif isinstance(result, Exception):
                    self.logger.warning(f"å­åŸŸåå‘ç°å¼‚å¸¸: {result}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åç»­å¤„ç†
            subdomain_list = list(all_subdomains)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ å¯è®¿é—®æ€§éªŒè¯
            if subdomain_list and config.get('verify_accessibility', True):
                self.logger.info(f"å¼€å§‹éªŒè¯ {len(subdomain_list)} ä¸ªå­åŸŸåçš„å¯è®¿é—®æ€§...")
                
                # ä½¿ç”¨å­åŸŸåå‘ç°å¼•æ“çš„å¯è®¿é—®æ€§éªŒè¯æ–¹æ³•
                verified_subdomains = await self.subdomain_engine._verify_accessibility(subdomain_list)
                self.results['subdomains'] = verified_subdomains
                
                # ç»Ÿè®¡å¯è®¿é—®çš„å­åŸŸå
                accessible_count = sum(1 for sub in verified_subdomains if sub.is_accessible)
                self.logger.info(f"å¯è®¿é—®æ€§éªŒè¯å®Œæˆ: {accessible_count}/{len(verified_subdomains)} ä¸ªå­åŸŸåå¯è®¿é—®")
            else:
                # å¦‚æœè·³è¿‡éªŒè¯ï¼Œç›´æ¥ä½¿ç”¨å‘ç°ç»“æœ
                self.results['subdomains'] = subdomain_list
                self.logger.warning("è·³è¿‡å¯è®¿é—®æ€§éªŒè¯")
            
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'subdomains_discovered',
                {
                    'count': len(self.results['subdomains']),
                    'domains': [sub.subdomain for sub in self.results['subdomains']][:10]  # å‰10ä¸ª
                }
            )
            
            # å°†å¯è®¿é—®çš„å­åŸŸåå‘é€åˆ°çˆ¬å–è½¨
            accessible_subdomains = [sub for sub in self.results['subdomains'] if sub.is_accessible]
            self.logger.info(f"å‘é€ {len(accessible_subdomains)} ä¸ªå¯è®¿é—®å­åŸŸååˆ°çˆ¬å–è½¨")
            
            for subdomain in accessible_subdomains:
                await self.discovery_to_crawl.put(subdomain)
            
            # å‘é€å®Œæˆä¿¡å·
            await self.discovery_to_crawl.put(None)
            
        except Exception as e:
            self.logger.error(f"å‘ç°è½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _crawling_pipeline(self, config: Dict[str, Any]):
        """çˆ¬å–è½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_started',
                {'stage': 'link_crawling'}
            )
            
            crawl_count = 0
            while True:
                if self.is_cancelled:
                    break
                
                # ä»å‘ç°è½¨æ¥æ”¶å­åŸŸå
                subdomain = await self.discovery_to_crawl.get()
                
                if subdomain is None:  # å®Œæˆä¿¡å·
                    break
                
                try:
                    # çˆ¬å–å•ä¸ªå­åŸŸå
                    crawl_results = await self._enhanced_crawl_subdomain(subdomain, config)
                    self.results['crawl_results'].extend(crawl_results)
                    
                    # å‘é€åˆ°åˆ†æè½¨
                    for crawl_result in crawl_results:
                        await self.crawl_to_analysis.put(crawl_result)
                    
                    crawl_count += 1
                    
                    await self.event_store.emit(
                        PipelineStage.CRAWLING,
                        'subdomain_crawled',
                        {
                            'subdomain': subdomain.subdomain,
                            'pages': len(crawl_results),
                            'total_crawled': crawl_count
                        }
                    )
                    
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å­åŸŸåå¤±è´¥ {subdomain.subdomain}: {e}")
                
                finally:
                    self.discovery_to_crawl.task_done()
            
            # å‘é€å®Œæˆä¿¡å·
            await self.crawl_to_analysis.put(None)
            
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_completed',
                {'total_crawled': crawl_count}
            )
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _analysis_pipeline(self, config: Dict[str, Any]):
        """åˆ†æè½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_started',
                {'stage': 'ai_analysis'}
            )
            
            analysis_count = 0
            ai_call_count = 0
            ai_skip_count = 0
            
            while True:
                if self.is_cancelled:
                    break
                
                # ä»çˆ¬å–è½¨æ¥æ”¶çˆ¬å–ç»“æœ
                crawl_result = await self.crawl_to_analysis.get()
                
                if crawl_result is None:  # å®Œæˆä¿¡å·
                    break
                
                try:
                    # ç¬¬ä¸‰æ–¹åŸŸåè¯†åˆ«
                    third_party_domains = await self.identifier_engine.identify_third_party_domains(
                        crawl_result.domain, [crawl_result], config
                    )
                    self.results['third_party_domains'].extend(third_party_domains)
                    
                    # å†…å®¹æŠ“å–
                    content_results = await self.capture_engine.capture_domain_content(
                        crawl_result.domain, [crawl_result.url], config
                    )
                    self.results['content_results'].extend(content_results)
                    
                    # æ™ºèƒ½AIåˆ†æï¼ˆé¢„ç­›é€‰ï¼‰
                    for content_result in content_results:
                        should_analyze, reason = await self._should_analyze_with_ai(content_result)
                        
                        if should_analyze:
                            ai_call_count += 1
                            violations = await self._perform_ai_analysis(content_result, config)
                            self.results['violation_records'].extend(violations)
                        else:
                            ai_skip_count += 1
                            await self.event_store.emit(
                                PipelineStage.ANALYSIS,
                                'ai_analysis_skipped',
                                {'reason': reason, 'url': content_result.url}
                            )
                    
                    analysis_count += 1
                    
                    await self.event_store.emit(
                        PipelineStage.ANALYSIS,
                        'domain_analyzed',
                        {
                            'domain': crawl_result.domain,
                            'third_party_count': len(third_party_domains),
                            'content_count': len(content_results),
                            'total_analyzed': analysis_count
                        }
                    )
                    
                except Exception as e:
                    self.logger.warning(f"åˆ†æåŸŸåå¤±è´¥ {crawl_result.domain}: {e}")
                
                finally:
                    self.crawl_to_analysis.task_done()
            
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_completed',
                {
                    'total_analyzed': analysis_count,
                    'ai_calls': ai_call_count,
                    'ai_skips': ai_skip_count,
                    'ai_efficiency': f"{(ai_skip_count / (ai_call_count + ai_skip_count) * 100):.1f}%" if (ai_call_count + ai_skip_count) > 0 else "0%"
                }
            )
            
        except Exception as e:
            self.logger.error(f"åˆ†æè½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _enhanced_dns_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """å¢å¼ºçš„DNSå‘ç°"""
        concurrency = config.get('dns_concurrency', 100)  # æé«˜å¹¶å‘åº¦
        timeout = config.get('dns_timeout', 3)  # é™ä½è¶…æ—¶æ—¶é—´
        
        return await self.subdomain_engine.discover_dns_with_concurrency(
            domain, concurrency, timeout
        )
    
    async def _enhanced_certificate_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """å¢å¼ºçš„è¯ä¹¦é€æ˜æ—¥å¿—å‘ç°"""
        # å¤šæºå¹¶å‘æŸ¥è¯¢
        sources = ['crt.sh', 'censys', 'facebook_ct']
        
        return await self.subdomain_engine.discover_certificate_multi_source(
            domain, sources
        )
    
    async def _passive_dns_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """è¢«åŠ¨DNSå‘ç°"""
        # ä»ç¬¬ä¸‰æ–¹æ•°æ®æºæŸ¥è¯¢
        return await self.subdomain_engine.discover_passive_dns(domain)
    
    async def _enhanced_crawl_subdomain(self, subdomain: SubdomainResult, config: Dict[str, Any]) -> List[CrawlResult]:
        """å¢å¼ºçš„å­åŸŸåçˆ¬å–"""
        max_pages = config.get('max_pages_per_subdomain', 20)
        
        # æ„å»ºèµ·å§‹URLåˆ—è¡¨
        start_urls = [
            f"https://{subdomain.subdomain}",
            f"http://{subdomain.subdomain}"
        ]
        
        # ä½¿ç”¨å®é™…å­˜åœ¨çš„crawl_domainæ–¹æ³•
        crawl_config = config.copy()
        crawl_config['max_pages_per_domain'] = max_pages
        
        return await self.crawler_engine.crawl_domain(
            subdomain.subdomain, start_urls, crawl_config
        )
    
    async def _should_analyze_with_ai(self, content_result: ContentResult) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦AIåˆ†æï¼ˆé¢„ç­›é€‰ï¼‰"""
        # å¿«é€Ÿé¢„ç­›é€‰è§„åˆ™
        if not content_result.screenshot_path:
            return False, "no_screenshot"
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        try:
            import os
            file_size = os.path.getsize(content_result.screenshot_path)
            if file_size < 1024:  # å°äº1KBï¼Œå¯èƒ½æ˜¯ç©ºæˆªå›¾
                return False, "screenshot_too_small"
        except:
            return False, "screenshot_file_error"
        
        # æ£€æŸ¥URLæ¨¡å¼
        suspicious_patterns = [
            'login', 'admin', 'auth', 'api', 'upload', 'download',
            'casino', 'porn', 'adult', 'gambling', 'phishing'
        ]
        
        url_lower = content_result.url.lower()
        for pattern in suspicious_patterns:
            if pattern in url_lower:
                return True, f"suspicious_pattern_{pattern}"
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if hasattr(content_result, 'status_code') and content_result.status_code is not None and content_result.status_code >= 400:
            return False, "error_status_code"
        
        # é»˜è®¤ï¼šéšæœºé‡‡æ ·ç­–ç•¥ï¼ˆé™ä½AIè°ƒç”¨ï¼‰
        import random
        sample_rate = 0.3  # 30%é‡‡æ ·ç‡
        if random.random() < sample_rate:
            return True, "random_sample"
        else:
            return False, "random_skip"
    
    async def _perform_ai_analysis(self, content_result: ContentResult, config: Dict[str, Any]) -> List[ViolationRecord]:
        """æ‰§è¡ŒAIåˆ†æ"""
        if not self.ai_engine:
            from app.models.user import UserAIConfig
            from app.core.database import AsyncSessionLocal
            
            async with AsyncSessionLocal() as db:
                ai_config = await db.get(UserAIConfig, self.user_id)
                if ai_config:
                    from app.engines.ai_analysis import AIAnalysisEngine
                    self.ai_engine = AIAnalysisEngine(self.task_id, ai_config)
        
        if self.ai_engine:
            # ç”±äºAIAnalysisEngine.analyze_domainséœ€è¦ThirdPartyDomainå¯¹è±¡åˆ—è¡¨
            # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„åŸŸåå¯¹è±¡æ¥è¿›è¡Œåˆ†æ
            from app.models.task import ThirdPartyDomain
            from urllib.parse import urlparse
            
            # ä»content_result.urlè§£æåŸŸå
            parsed_url = urlparse(content_result.url)
            domain_name = parsed_url.netloc
            
            # åˆ›å»ºä¸´æ—¶çš„ThirdPartyDomainå¯¹è±¡
            temp_domain = ThirdPartyDomain(
                task_id=self.task_id,
                domain=domain_name,
                found_on_url=content_result.url,
                screenshot_path=content_result.screenshot_path,
                page_title=getattr(content_result, 'page_title', ''),
                page_description=getattr(content_result, 'page_description', ''),
                domain_type='unknown',
                is_analyzed=False
            )
            
            # è°ƒç”¨analyze_domainsæ–¹æ³•
            violations = await self.ai_engine.analyze_domains([temp_domain])
            return violations
        else:
            return []
    
    async def _calculate_final_statistics(self):
        """è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        self.results['statistics'] = {
            'total_subdomains': len(self.results['subdomains']),
            'accessible_subdomains': len([s for s in self.results['subdomains'] if s.is_accessible]),
            'total_pages_crawled': len(self.results['crawl_results']),
            'total_third_party_domains': len(self.results['third_party_domains']),
            'total_violations': len(self.results['violation_records']),
            'execution_duration': int(time.time() - self.start_time) if self.start_time else 0,
            'pipeline_efficiency': {
                'discovery_queue_max': self.discovery_to_crawl.processed_count,
                'crawl_queue_max': self.crawl_to_analysis.processed_count,
            }
        }
    
    def _build_scan_result(self) -> Dict[str, Any]:
        """æ„å»ºæ‰«æç»“æœ"""
        return {
            'task_id': self.task_id,
            'status': 'completed' if not self.is_cancelled else 'cancelled',
            'start_time': self.start_time,
            'end_time': self.end_time,
            'events': self.event_store.get_events(),
            'results': self.results
        }
    
    async def cancel_scan(self):
        """å–æ¶ˆæ‰«æ"""
        self.is_cancelled = True
        await self.event_store.emit(
            PipelineStage.ANALYSIS,
            'scan_cancelled',
            {'reason': 'user_request'}
        )
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'is_cancelled': self.is_cancelled,
            'task_id': self.task_id,
            'queue_status': {
                'discovery_to_crawl': self.discovery_to_crawl.qsize(),
                'crawl_to_analysis': self.crawl_to_analysis.qsize()
            },
            'events_count': len(self.event_store.events)
        }