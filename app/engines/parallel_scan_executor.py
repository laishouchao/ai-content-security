import asyncio
import time
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, asdict
from enum import Enum

from app.core.logging import TaskLogger
from app.engines.subdomain_discovery import SubdomainDiscoveryEngine, SubdomainResult
from app.engines.link_crawler import LinkCrawlerEngine, CrawlResult
from app.engines.third_party_identifier import ThirdPartyIdentifierEngine, ThirdPartyDomainResult
from app.engines.content_capture import ContentCaptureEngine, ContentResult
from app.engines.ai_analysis import AIAnalysisEngine
from app.models.task import ViolationRecord


class PipelineStage(Enum):
    """æµæ°´çº¿é˜¶æ®µ"""
    DISCOVERY = "discovery"
    CRAWLING = "crawling"
    ANALYSIS = "analysis"


@dataclass
class ScanEvent:
    """æ‰«æäº‹ä»¶"""
    timestamp: float
    stage: PipelineStage
    event_type: str
    data: Dict[str, Any]
    task_id: str
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp,
            'stage': self.stage.value,
            'event_type': self.event_type,
            'data': self.data,
            'task_id': self.task_id
        }


class EventStore:
    """äº‹ä»¶å­˜å‚¨å™¨"""
    
    def __init__(self, task_id: str):
        self.task_id = task_id
        self.events: List[ScanEvent] = []
        self.subscribers = []
    
    async def emit(self, stage: PipelineStage, event_type: str, data: Dict[str, Any]):
        """å‘å°„äº‹ä»¶"""
        event = ScanEvent(
            timestamp=time.time(),
            stage=stage,
            event_type=event_type,
            data=data,
            task_id=self.task_id
        )
        
        self.events.append(event)
        
        # é€šçŸ¥æ‰€æœ‰è®¢é˜…è€…
        for subscriber in self.subscribers:
            try:
                await subscriber(event)
            except Exception as e:
                print(f"Event subscriber error: {e}")
    
    def subscribe(self, callback):
        """è®¢é˜…äº‹ä»¶"""
        self.subscribers.append(callback)
    
    def get_events(self) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰äº‹ä»¶"""
        return [event.to_dict() for event in self.events]


class PipelineQueue:
    """è½¨é—´é€šä¿¡é˜Ÿåˆ—"""
    
    def __init__(self, maxsize: int = 1000):
        self.queue = asyncio.Queue(maxsize=maxsize)
        self.processed_count = 0
        self.error_count = 0
    
    async def put(self, item: Any):
        """æ”¾å…¥é˜Ÿåˆ—"""
        await self.queue.put(item)
    
    async def get(self) -> Any:
        """ä»é˜Ÿåˆ—è·å–"""
        item = await self.queue.get()
        self.processed_count += 1
        return item
    
    def qsize(self) -> int:
        """é˜Ÿåˆ—å¤§å°"""
        return self.queue.qsize()
    
    def task_done(self):
        """æ ‡è®°ä»»åŠ¡å®Œæˆ"""
        self.queue.task_done()


class ParallelScanExecutor:
    """å¹¶è¡Œæ‰«ææ‰§è¡Œå™¨"""
    
    def __init__(self, task_id: str, user_id: str):
        self.task_id = task_id
        self.user_id = user_id
        self.logger = TaskLogger(task_id, user_id)
        
        # äº‹ä»¶å­˜å‚¨
        self.event_store = EventStore(task_id)
        
        # å¼•æ“å®ä¾‹
        self.subdomain_engine = SubdomainDiscoveryEngine(task_id, user_id)
        self.crawler_engine = LinkCrawlerEngine(task_id, user_id)
        self.identifier_engine = ThirdPartyIdentifierEngine(task_id, user_id)
        self.capture_engine = ContentCaptureEngine(task_id, user_id)
        self.ai_engine = None  # å»¶è¿Ÿåˆå§‹åŒ–
        
        # è½¨é—´é˜Ÿåˆ—
        self.discovery_to_crawl = PipelineQueue(maxsize=2000)
        self.crawl_to_analysis = PipelineQueue(maxsize=2000)
        
        # æ‰§è¡ŒçŠ¶æ€
        self.is_running = False
        self.is_cancelled = False
        self.start_time = None
        self.end_time = None
        
        # ç»“æœæ”¶é›†
        self.results = {
            'subdomains': [],
            'crawl_results': [],
            'domain_records': [],
            'content_results': [],
            'violation_records': [],
            'statistics': {}
        }
    
    async def execute_scan(self, target_domain: str, config: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå¹¶è¡Œæ‰«æ"""
        self.is_running = True
        self.is_cancelled = False
        self.start_time = time.time()
        
        try:
            await self.event_store.emit(
                PipelineStage.DISCOVERY, 
                'scan_started', 
                {'target_domain': target_domain, 'config': config}
            )
            
            # å¯åŠ¨ä¸‰è½¨å¹¶è¡Œæµæ°´çº¿
            discovery_task = asyncio.create_task(
                self._discovery_pipeline(target_domain, config)
            )
            crawl_task = asyncio.create_task(
                self._crawling_pipeline(config)
            )
            analysis_task = asyncio.create_task(
                self._analysis_pipeline(config)
            )
            
            # ç­‰å¾…æ‰€æœ‰è½¨å®Œæˆ
            await asyncio.gather(
                discovery_task, 
                crawl_task, 
                analysis_task,
                return_exceptions=True
            )
            
            # è®¡ç®—æœ€ç»ˆç»Ÿè®¡
            await self._calculate_final_statistics()
            
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'scan_completed',
                {'duration': time.time() - self.start_time}
            )
            
            return self._build_scan_result()
            
        except Exception as e:
            self.logger.error(f"å¹¶è¡Œæ‰«ææ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'scan_failed',
                {'error': str(e)}
            )
            raise
        finally:
            self.is_running = False
            self.end_time = time.time()
    
    async def _discovery_pipeline(self, target_domain: str, config: Dict[str, Any]):
        """å‘ç°è½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'stage_started',
                {'stage': 'subdomain_discovery'}
            )
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å­åŸŸåå‘ç°æ–¹æ³•
            discovery_tasks = []
            
            # DNSçˆ†ç ´ï¼ˆé«˜å¹¶å‘ï¼‰
            if config.get('subdomain_discovery_enabled', True):
                discovery_tasks.append(
                    self._enhanced_dns_discovery(target_domain, config)
                )
            
            # è¯ä¹¦é€æ˜æ—¥å¿—ï¼ˆå¤šæºï¼‰
            if config.get('certificate_discovery_enabled', True):
                discovery_tasks.append(
                    self._enhanced_certificate_discovery(target_domain, config)
                )
            
            # è¢«åŠ¨DNS
            if config.get('passive_dns_enabled', True):
                discovery_tasks.append(
                    self._passive_dns_discovery(target_domain, config)
                )
            
            # å¹¶è¡Œæ‰§è¡Œæ‰€æœ‰å‘ç°ä»»åŠ¡
            discovery_results = await asyncio.gather(*discovery_tasks, return_exceptions=True)
            
            # åˆå¹¶å»é‡ç»“æœ
            all_subdomains = set()
            for result in discovery_results:
                if isinstance(result, list):
                    all_subdomains.update(result)
                elif isinstance(result, Exception):
                    self.logger.warning(f"å­åŸŸåå‘ç°å¼‚å¸¸: {result}")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨ä»¥ä¾¿åç»­å¤„ç†
            subdomain_list = list(all_subdomains)
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ·»åŠ å¯è®¿é—®æ€§éªŒè¯
            if subdomain_list and config.get('verify_accessibility', True):
                self.logger.info(f"å¼€å§‹éªŒè¯ {len(subdomain_list)} ä¸ªå­åŸŸåçš„å¯è®¿é—®æ€§...")
                
                # ä½¿ç”¨å­åŸŸåå‘ç°å¼•æ“çš„å¯è®¿é—®æ€§éªŒè¯æ–¹æ³•
                verified_subdomains = await self.subdomain_engine._verify_accessibility(subdomain_list)
                self.results['subdomains'] = verified_subdomains
                
                # ç»Ÿè®¡å¯è®¿é—®çš„å­åŸŸå
                accessible_count = sum(1 for sub in verified_subdomains if sub.is_accessible)
                self.logger.info(f"å¯è®¿é—®æ€§éªŒè¯å®Œæˆ: {accessible_count}/{len(verified_subdomains)} ä¸ªå­åŸŸåå¯è®¿é—®")
            else:
                # å¦‚æœè·³è¿‡éªŒè¯ï¼Œç›´æ¥ä½¿ç”¨å‘ç°ç»“æœ
                self.results['subdomains'] = subdomain_list
                self.logger.warning("è·³è¿‡å¯è®¿é—®æ€§éªŒè¯")
            
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'subdomains_discovered',
                {
                    'count': len(self.results['subdomains']),
                    'domains': [sub.subdomain for sub in self.results['subdomains']][:10]  # å‰10ä¸ª
                }
            )
            
            # å°†å¯è®¿é—®çš„å­åŸŸåå‘é€åˆ°çˆ¬å–è½¨
            accessible_subdomains = [sub for sub in self.results['subdomains'] if sub.is_accessible]
            self.logger.info(f"å‘é€ {len(accessible_subdomains)} ä¸ªå¯è®¿é—®å­åŸŸååˆ°çˆ¬å–è½¨")
            
            for subdomain in accessible_subdomains:
                await self.discovery_to_crawl.put(subdomain)
            
            # å‘é€å®Œæˆä¿¡å·
            await self.discovery_to_crawl.put(None)
            
        except Exception as e:
            self.logger.error(f"å‘ç°è½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.DISCOVERY,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _crawling_pipeline(self, config: Dict[str, Any]):
        """çˆ¬å–è½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_started',
                {'stage': 'link_crawling'}
            )
            
            crawl_count = 0
            while True:
                if self.is_cancelled:
                    break
                
                # ä»å‘ç°è½¨æ¥æ”¶å­åŸŸå
                subdomain = await self.discovery_to_crawl.get()
                
                if subdomain is None:  # å®Œæˆä¿¡å·
                    break
                
                try:
                    # çˆ¬å–å•ä¸ªå­åŸŸå
                    crawl_results = await self._enhanced_crawl_subdomain(subdomain, config)
                    self.results['crawl_results'].extend(crawl_results)
                    
                    # å‘é€åˆ°åˆ†æè½¨
                    for crawl_result in crawl_results:
                        await self.crawl_to_analysis.put(crawl_result)
                    
                    crawl_count += 1
                    
                    await self.event_store.emit(
                        PipelineStage.CRAWLING,
                        'subdomain_crawled',
                        {
                            'subdomain': subdomain.subdomain,
                            'pages': len(crawl_results),
                            'total_crawled': crawl_count
                        }
                    )
                    
                except Exception as e:
                    self.logger.warning(f"çˆ¬å–å­åŸŸåå¤±è´¥ {subdomain.subdomain}: {e}")
                
                finally:
                    self.discovery_to_crawl.task_done()
            
            # å‘é€å®Œæˆä¿¡å·
            await self.crawl_to_analysis.put(None)
            
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_completed',
                {'total_crawled': crawl_count}
            )
            
        except Exception as e:
            self.logger.error(f"çˆ¬å–è½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.CRAWLING,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _analysis_pipeline(self, config: Dict[str, Any]):
        """åˆ†æè½¨æµæ°´çº¿"""
        try:
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_started',
                {'stage': 'ai_analysis'}
            )
            
            # æ£€æŸ¥AIåˆ†ææ˜¯å¦å¯ç”¨
            ai_analysis_enabled = config.get('ai_analysis_enabled', True)
            self.logger.info(f"AIåˆ†æé…ç½®: ai_analysis_enabled={ai_analysis_enabled}")
            
            if not ai_analysis_enabled:
                self.logger.warning("âš ï¸ AIåˆ†æå·²è¢«ç¦ç”¨ï¼Œè·³è¿‡åˆ†æé˜¶æ®µ")
                await self.event_store.emit(
                    PipelineStage.ANALYSIS,
                    'ai_analysis_disabled',
                    {'reason': 'ai_analysis_disabled_in_config'}
                )
                return
            
            analysis_count = 0
            ai_call_count = 0
            ai_skip_count = 0
            
            while True:
                if self.is_cancelled:
                    break
                
                # ä»çˆ¬å–è½¨æ¥æ”¶çˆ¬å–ç»“æœ
                crawl_result = await self.crawl_to_analysis.get()
                
                if crawl_result is None:  # å®Œæˆä¿¡å·
                    break
                
                try:
                    self.logger.info(f"ğŸ” å¼€å§‹åˆ†æåŸŸå: {crawl_result.domain}")
                    
                    # ç¬¬ä¸‰æ–¹åŸŸåè¯†åˆ«
                    domain_records = await self.identifier_engine.identify_domain_records(
                        crawl_result.domain, [crawl_result], config
                    )
                    self.results['domain_records'].extend(domain_records)
                    self.logger.info(f"è¯†åˆ«åˆ° {len(domain_records)} ä¸ªç¬¬ä¸‰æ–¹åŸŸå")
                    
                    # å†…å®¹æŠ“å–
                    content_results = await self.capture_engine.capture_domain_content(
                        crawl_result.domain, [crawl_result.url], config
                    )
                    self.results['content_results'].extend(content_results)
                    self.logger.info(f"æŠ“å–åˆ° {len(content_results)} ä¸ªå†…å®¹ç»“æœ")
                    
                    # AIåˆ†æ
                    for i, content_result in enumerate(content_results):
                        self.logger.info(f"ğŸ¤– æ­£åœ¨å¤„ç†å†…å®¹ ({i+1}/{len(content_results)}): {content_result.url}")
                        
                        # è¯¦ç»†æ£€æŸ¥å†…å®¹ç»“æœ
                        self.logger.debug(f"å†…å®¹ç»“æœè¯¦æƒ…: screenshot_path={getattr(content_result, 'screenshot_path', None)}, "
                                         f"status_code={getattr(content_result, 'status_code', None)}")
                        
                        should_analyze, reason = await self._should_analyze_with_ai(content_result)
                        self.logger.info(f"åˆ†ææ£€æŸ¥ç»“æœ: should_analyze={should_analyze}, reason={reason}")
                        
                        if should_analyze:
                            ai_call_count += 1
                            self.logger.info(f"âœ… æ‰§è¡ŒAIåˆ†æ (#{ai_call_count}): {content_result.url}")
                            
                            try:
                                violations = await self._perform_ai_analysis(content_result, config)
                                self.results['violation_records'].extend(violations)
                                self.logger.info(f"ğŸš¨ AIåˆ†æå®Œæˆï¼Œå‘ç° {len(violations)} ä¸ªè¿è§„")
                            except Exception as ai_error:
                                self.logger.error(f"âŒ AIåˆ†æå¤±è´¥: {ai_error}")
                                await self.event_store.emit(
                                    PipelineStage.ANALYSIS,
                                    'ai_analysis_error',
                                    {'url': content_result.url, 'error': str(ai_error)}
                                )
                        else:
                            ai_skip_count += 1
                            self.logger.info(f"â­ï¸ è·³è¿‡AIåˆ†æ (#{ai_skip_count}): {reason} - {content_result.url}")
                            await self.event_store.emit(
                                PipelineStage.ANALYSIS,
                                'ai_analysis_skipped',
                                {'reason': reason, 'url': content_result.url}
                            )
                    
                    analysis_count += 1
                    
                    await self.event_store.emit(
                        PipelineStage.ANALYSIS,
                        'domain_analyzed',
                        {
                            'domain': crawl_result.domain,
                            'third_party_count': len(domain_records),
                            'content_count': len(content_results),
                            'total_analyzed': analysis_count
                        }
                    )
                    
                except Exception as e:
                    self.logger.warning(f"åˆ†æåŸŸåå¤±è´¥ {crawl_result.domain}: {e}")
                
                finally:
                    self.crawl_to_analysis.task_done()
            
            # è®¡ç®—AIæ•ˆç‡ç»Ÿè®¡
            ai_efficiency = 0
            if (ai_call_count + ai_skip_count) > 0:
                ai_efficiency = (ai_skip_count / (ai_call_count + ai_skip_count) * 100)
            
            self.logger.info(f"ğŸ åˆ†æé˜¶æ®µå®Œæˆç»Ÿè®¡:")
            self.logger.info(f"  - æ€»åˆ†æåŸŸå: {analysis_count}")
            self.logger.info(f"  - AIè°ƒç”¨æ¬¡æ•°: {ai_call_count}")
            self.logger.info(f"  - AIè·³è¿‡æ¬¡æ•°: {ai_skip_count}")
            self.logger.info(f"  - AIæ•ˆç‡: {ai_efficiency:.1f}%")
            
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_completed',
                {
                    'total_analyzed': analysis_count,
                    'ai_calls': ai_call_count,
                    'ai_skips': ai_skip_count,
                    'ai_efficiency': f"{ai_efficiency:.1f}%"
                }
            )
            
        except Exception as e:
            self.logger.error(f"åˆ†æè½¨æ‰§è¡Œå¤±è´¥: {e}")
            await self.event_store.emit(
                PipelineStage.ANALYSIS,
                'stage_failed',
                {'error': str(e)}
            )
    
    async def _enhanced_dns_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """å¢å¼ºçš„DNSå‘ç°"""
        concurrency = config.get('dns_concurrency', 100)  # æé«˜å¹¶å‘åº¦
        timeout = config.get('dns_timeout', 3)  # é™ä½è¶…æ—¶æ—¶é—´
        
        return await self.subdomain_engine.discover_dns_with_concurrency(
            domain, concurrency, timeout
        )
    
    async def _enhanced_certificate_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """å¢å¼ºçš„è¯ä¹¦é€æ˜æ—¥å¿—å‘ç°"""
        # å¤šæºå¹¶å‘æŸ¥è¯¢
        sources = ['crt.sh', 'censys', 'facebook_ct']
        
        return await self.subdomain_engine.discover_certificate_multi_source(
            domain, sources
        )
    
    async def _passive_dns_discovery(self, domain: str, config: Dict[str, Any]) -> List[SubdomainResult]:
        """è¢«åŠ¨DNSå‘ç°"""
        # ä»ç¬¬ä¸‰æ–¹æ•°æ®æºæŸ¥è¯¢
        return await self.subdomain_engine.discover_passive_dns(domain)
    
    async def _enhanced_crawl_subdomain(self, subdomain: SubdomainResult, config: Dict[str, Any]) -> List[CrawlResult]:
        """å¢å¼ºçš„å­åŸŸåçˆ¬å–"""
        max_pages = config.get('max_pages_per_subdomain', 20)
        
        # æ„å»ºèµ·å§‹URLåˆ—è¡¨
        start_urls = [
            f"https://{subdomain.subdomain}",
            f"http://{subdomain.subdomain}"
        ]
        
        # ä½¿ç”¨å®é™…å­˜åœ¨çš„crawl_domainæ–¹æ³•
        crawl_config = config.copy()
        crawl_config['max_pages_per_domain'] = max_pages
        
        return await self.crawler_engine.crawl_domain(
            subdomain.subdomain, start_urls, crawl_config
        )
    
    async def _should_analyze_with_ai(self, content_result: ContentResult) -> Tuple[bool, str]:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦AIåˆ†æ"""
        self.logger.debug(f"ğŸ” å¼€å§‹åˆ†ææ£€æŸ¥: {content_result.url}")
        
        # æ£€æŸ¥æˆªå›¾æ–‡ä»¶
        if not content_result.screenshot_path:
            self.logger.warning(f"âš ï¸ æ²¡æœ‰æˆªå›¾è·¯å¾„: {content_result.url}")
            return False, "no_screenshot"
        
        self.logger.debug(f"ğŸ–¼ï¸ æˆªå›¾è·¯å¾„: {content_result.screenshot_path}")
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        try:
            import os
            if not os.path.exists(content_result.screenshot_path):
                self.logger.warning(f"âš ï¸ æˆªå›¾æ–‡ä»¶ä¸å­˜åœ¨: {content_result.screenshot_path}")
                return False, "screenshot_file_not_exists"
                
            file_size = os.path.getsize(content_result.screenshot_path)
            self.logger.debug(f"ğŸ“„ æˆªå›¾æ–‡ä»¶å¤§å°: {file_size} bytes")
            
            if file_size < 1024:  # å°äº1KBï¼Œå¯èƒ½æ˜¯ç©ºæˆªå›¾
                self.logger.warning(f"âš ï¸ æˆªå›¾æ–‡ä»¶å¤ªå°: {file_size} bytes < 1KB")
                return False, "screenshot_too_small"
        except Exception as e:
            self.logger.error(f"âŒ æ£€æŸ¥æˆªå›¾æ–‡ä»¶å¤±è´¥: {e}")
            return False, "screenshot_file_error"
        
        # æ£€æŸ¥çŠ¶æ€ç 
        if hasattr(content_result, 'status_code') and content_result.status_code is not None and content_result.status_code >= 400:
            self.logger.info(f"âš ï¸ é”™è¯¯çŠ¶æ€ç : {content_result.status_code}")
            return False, "error_status_code"
        
        # å¯¹æ‰€æœ‰æœ‰æ•ˆå†…å®¹è¿›è¡ŒAIåˆ†æ
        self.logger.info(f"âœ… å¯¹æ‰€æœ‰å†…å®¹è¿›è¡ŒAIåˆ†æ: {content_result.url}")
        return True, "analyze_all_content"
    
    async def _perform_ai_analysis(self, content_result: ContentResult, config: Dict[str, Any]) -> List[ViolationRecord]:
        """æ‰§è¡ŒAIåˆ†æ"""
        self.logger.info(f"ğŸ¤– å¼€å§‹æ‰§è¡ŒAIåˆ†æ: {content_result.url}")
        
        # æ£€æŸ¥AIå¼•æ“æ˜¯å¦å·²åˆå§‹åŒ–
        if not self.ai_engine:
            self.logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–AIå¼•æ“...")
            
            from app.models.user import UserAIConfig
            from app.core.database import AsyncSessionLocal
            from sqlalchemy import select
            from app.core.security import data_encryption
            
            try:
                async with AsyncSessionLocal() as db:
                    # ä½¿ç”¨æ­£ç¡®çš„æŸ¥è¯¢æ–¹å¼ï¼šæ ¹æ®user_idæŸ¥è¯¢ï¼Œè€Œä¸æ˜¯ä½¿ç”¨ä¸»é”®
                    stmt = select(UserAIConfig).where(UserAIConfig.user_id == self.user_id)
                    result = await db.execute(stmt)
                    ai_config = result.scalar_one_or_none()
                    
                    if not ai_config:
                        self.logger.error(f"âŒ ç”¨æˆ·AIé…ç½®ä¸å­˜åœ¨: user_id={self.user_id}")
                        return []
                    
                    # è§£å¯†APIå¯†é’¥
                    if ai_config.openai_api_key is not None:
                        try:
                            decrypted_api_key = data_encryption.decrypt_data(str(ai_config.openai_api_key))
                            # ä½¿ç”¨setattræ¥é¿å…SQLAlchemy Columnç±»å‹æ£€æŸ¥é—®é¢˜
                            setattr(ai_config, 'openai_api_key', decrypted_api_key)
                        except Exception as e:
                            self.logger.warning(f"è§£å¯†APIå¯†é’¥å¤±è´¥: {e}")
                            # ä½¿ç”¨setattræ¥é¿å…SQLAlchemy Columnç±»å‹æ£€æŸ¥é—®é¢˜
                            setattr(ai_config, 'openai_api_key', None)
                    
                    self.logger.debug(f"ğŸ”‘ è·å–åˆ°AIé…ç½®: model={ai_config.model_name}, "
                                     f"has_api_key={bool(ai_config.openai_api_key)}, "
                                     f"has_valid_config={ai_config.has_valid_config}")
                    
                    if not ai_config.has_valid_config:
                        self.logger.error(f"âŒ ç”¨æˆ·AIé…ç½®æ— æ•ˆ: user_id={self.user_id}")
                        return []
                    
                    from app.engines.ai_analysis import AIAnalysisEngine
                    self.ai_engine = AIAnalysisEngine(self.task_id, ai_config)
                    self.logger.info("âœ… AIå¼•æ“åˆå§‹åŒ–æˆåŠŸ")
                    
            except Exception as e:
                self.logger.error(f"âŒ AIå¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
                return []
        
        if self.ai_engine:
            try:
                # ç”±äºAIAnalysisEngine.analyze_domainséœ€è¦ThirdPartyDomainå¯¹è±¡åˆ—è¡¨
                # æˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„åŸŸåå¯¹è±¡æ¥è¿›è¡Œåˆ†æ
                from app.models.domain import DomainRecord
                from urllib.parse import urlparse
                
                # ä»content_result.urlè§£æåŸŸå
                parsed_url = urlparse(content_result.url)
                domain_name = parsed_url.netloc
                
                self.logger.debug(f"ğŸŒ è§£æåŸŸå: {domain_name} from {content_result.url}")
                
                # åˆ›å»ºä¸´æ—¶çš„ThirdPartyDomainå¯¹è±¡
                temp_domain = DomainRecord(
                    task_id=self.task_id,
                    domain=domain_name,
                    found_on_url=content_result.url,
                    screenshot_path=content_result.screenshot_path,
                    page_title=getattr(content_result, 'page_title', ''),
                    page_description=getattr(content_result, 'page_description', ''),
                    domain_type='unknown',
                    is_analyzed=False
                )
                
                self.logger.info(f"ğŸ“¦ åˆ›å»ºä¸´æ—¶åŸŸåå¯¹è±¡: {domain_name}")
                
                # è°ƒç”¨analyze_domainsæ–¹æ³•
                self.logger.info(f"ğŸš€ å¼€å§‹è°ƒç”¨AIåˆ†ææ¥å£...")
                violations = await self.ai_engine.analyze_domains([temp_domain])
                
                self.logger.info(f"âœ… AIåˆ†æå®Œæˆï¼Œè¿”å› {len(violations)} ä¸ªè¿è§„è®°å½•")
                
                # è®°å½•è¿è§„è¯¦æƒ…
                for i, violation in enumerate(violations):
                    self.logger.info(f"ğŸš¨ è¿è§„#{i+1}: {violation.violation_type} - "
                                   f"ç½®ä¿¡åº¦:{violation.confidence_score:.2f} - "
                                   f"é£é™©ç­‰çº§:{violation.risk_level}")
                
                return violations
                
            except Exception as e:
                self.logger.error(f"âŒ AIåˆ†ææ‰§è¡Œå¤±è´¥: {e}")
                import traceback
                self.logger.error(f"é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
                return []
        else:
            self.logger.error("âŒ AIå¼•æ“ä¸å¯ç”¨")
            return []
    
    async def _calculate_final_statistics(self):
        """è®¡ç®—æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        self.results['statistics'] = {
            'total_subdomains': len(self.results['subdomains']),
            'accessible_subdomains': len([s for s in self.results['subdomains'] if s.is_accessible]),
            'total_pages_crawled': len(self.results['crawl_results']),
            'total_domain_records': len(self.results['domain_records']),
            'total_violations': len(self.results['violation_records']),
            'execution_duration': int(time.time() - self.start_time) if self.start_time else 0,
            'pipeline_efficiency': {
                'discovery_queue_max': self.discovery_to_crawl.processed_count,
                'crawl_queue_max': self.crawl_to_analysis.processed_count,
            }
        }
    
    def _build_scan_result(self) -> Dict[str, Any]:
        """æ„å»ºæ‰«æç»“æœ"""
        return {
            'task_id': self.task_id,
            'status': 'completed' if not self.is_cancelled else 'cancelled',
            'start_time': self.start_time,
            'end_time': self.end_time,
            'events': self.event_store.get_events(),
            'results': self.results
        }
    
    async def cancel_scan(self):
        """å–æ¶ˆæ‰«æ"""
        self.is_cancelled = True
        await self.event_store.emit(
            PipelineStage.ANALYSIS,
            'scan_cancelled',
            {'reason': 'user_request'}
        )
    
    def get_current_status(self) -> Dict[str, Any]:
        """è·å–å½“å‰çŠ¶æ€"""
        return {
            'is_running': self.is_running,
            'is_cancelled': self.is_cancelled,
            'task_id': self.task_id,
            'queue_status': {
                'discovery_to_crawl': self.discovery_to_crawl.qsize(),
                'crawl_to_analysis': self.crawl_to_analysis.qsize()
            },
            'events_count': len(self.event_store.events)
        }