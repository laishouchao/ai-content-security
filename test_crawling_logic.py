#!/usr/bin/env python3
"""
æµ‹è¯•çˆ¬å–é€»è¾‘çš„è„šæœ¬
éªŒè¯ä»é“¾æ¥ä¸­æå–å­åŸŸåå’Œç¬¬ä¸‰æ–¹åŸŸåçš„åŠŸèƒ½
"""

import asyncio
import sys
import os
from pathlib import Path
from urllib.parse import urlparse

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.engines.scan_executor import ScanTaskExecutor
from app.engines.link_crawler import LinkCrawlerEngine
from app.core.logging import TaskLogger


def _extract_subdomains_from_links(links: set[str], target_domain: str) -> set[str]:
    """ä»é“¾æ¥ä¸­æå–å±äºç›®æ ‡åŸŸåçš„å­åŸŸå"""
    subdomains = set()
    target_domain_lower = target_domain.lower()
    
    for link in links:
        try:
            parsed = urlparse(link)
            if parsed.netloc:
                domain = parsed.netloc.lower()
                
                # ç§»é™¤ç«¯å£å·
                if ':' in domain:
                    domain = domain.split(':')[0]
                
                # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡åŸŸåçš„å­åŸŸå
                if domain != target_domain_lower and domain.endswith(f'.{target_domain_lower}'):
                    subdomains.add(domain)
                    
        except Exception:
            continue
    
    return subdomains


def _extract_domain_from_url(url: str) -> str:
    """ä»URLä¸­æå–åŸŸå"""
    try:
        parsed = urlparse(url)
        if parsed.netloc:
            domain = parsed.netloc.lower()
            # ç§»é™¤ç«¯å£å·
            if ':' in domain:
                domain = domain.split(':')[0]
            return domain
    except Exception:
        pass
    return ""


def _is_target_domain_or_subdomain(url: str, target_domain: str) -> bool:
    """æ£€æŸ¥URLæ˜¯å¦å±äºç›®æ ‡åŸŸåæˆ–å…¶å­åŸŸå"""
    try:
        parsed = urlparse(url)
        if parsed.netloc:
            domain = parsed.netloc.lower()
            
            # ç§»é™¤ç«¯å£å·
            if ':' in domain:
                domain = domain.split(':')[0]
            
            target_domain_lower = target_domain.lower()
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡åŸŸåæˆ–å…¶å­åŸŸå
            return domain == target_domain_lower or domain.endswith(f'.{target_domain_lower}')
            
    except Exception:
        pass
    
    return False


async def test_subdomain_extraction():
    """æµ‹è¯•å­åŸŸåæå–åŠŸèƒ½"""
    print("=== æµ‹è¯•å­åŸŸåæå–åŠŸèƒ½ ===")
    
    # æµ‹è¯•é“¾æ¥é›†åˆ
    test_links = {
        "https://api.example.com/some/path",
        "https://cdn.example.com/assets/style.css",
        "https://blog.example.com/article/123",
        "https://external-service.com/api/data",
        "https://another-external.org/page",
        "https://sub.domain.example.com/nested/path",
        "https://example.com/main/page"
    }
    
    # æå–å­åŸŸå
    target_domain = "example.com"
    extracted_subdomains = _extract_subdomains_from_links(test_links, target_domain)
    
    print(f"ç›®æ ‡åŸŸå: {target_domain}")
    print(f"æµ‹è¯•é“¾æ¥: {len(test_links)} ä¸ª")
    print(f"æå–åˆ°çš„å­åŸŸå: {extracted_subdomains}")
    
    # éªŒè¯ç»“æœ
    expected_subdomains = {
        "api.example.com",
        "cdn.example.com", 
        "blog.example.com",
        "sub.domain.example.com"
    }
    
    if extracted_subdomains == expected_subdomains:
        print("âœ“ å­åŸŸåæå–åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âœ— å­åŸŸåæå–åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        print(f"  æœŸæœ›: {expected_subdomains}")
        print(f"  å®é™…: {extracted_subdomains}")
        return False


async def test_third_party_domain_extraction():
    """æµ‹è¯•ç¬¬ä¸‰æ–¹åŸŸåæå–åŠŸèƒ½"""
    print("\n=== æµ‹è¯•ç¬¬ä¸‰æ–¹åŸŸåæå–åŠŸèƒ½ ===")
    
    # æµ‹è¯•é“¾æ¥é›†åˆ
    test_links = {
        "https://api.example.com/some/path",
        "https://cdn.example.com/assets/style.css",
        "https://external-service.com/api/data",
        "https://another-external.org/page",
        "https://sub.domain.example.com/nested/path",
        "https://example.com/main/page",
        "https://suspicious-site.tk/malware",
        "https://analytics.google.com/collect"
    }
    
    # æ¨¡æ‹Ÿç¬¬ä¸‰æ–¹åŸŸåç»“æœåˆ—è¡¨
    third_party_domains = []
    
    # ä»é“¾æ¥ä¸­æå–ç¬¬ä¸‰æ–¹åŸŸå
    target_domain = "example.com"
    for link in test_links:
        if not _is_target_domain_or_subdomain(link, target_domain):
            third_party_domain = _extract_domain_from_url(link)
            if third_party_domain:
                # æ£€æŸ¥æ˜¯å¦å·²ç»å­˜åœ¨äºç¬¬ä¸‰æ–¹åŸŸåç»“æœä¸­
                existing_third_party = {d['domain'] for d in third_party_domains}
                if third_party_domain not in existing_third_party:
                    # åˆ›å»ºç¬¬ä¸‰æ–¹åŸŸåç»“æœ
                    third_party_result = {
                        'domain': third_party_domain,
                        'domain_type': "unknown",
                        'risk_level': "low",
                        'found_on_urls': [link],
                        'confidence_score': 0.5,
                        'description': f"ä»é“¾æ¥ä¸­å‘ç°çš„ç¬¬ä¸‰æ–¹åŸŸå: {third_party_domain}",
                        'category_tags': ["discovered_from_crawling"]
                    }
                    third_party_domains.append(third_party_result)
    
    print(f"ç›®æ ‡åŸŸå: {target_domain}")
    print(f"æµ‹è¯•é“¾æ¥: {len(test_links)} ä¸ª")
    print(f"æå–åˆ°çš„ç¬¬ä¸‰æ–¹åŸŸå: {len(third_party_domains)} ä¸ª")
    
    for domain in third_party_domains:
        print(f"  - {domain['domain']} (å‘ç°äº: {domain['found_on_urls'][0]})")
    
    # éªŒè¯ç»“æœ
    expected_third_party = {
        "external-service.com",
        "another-external.org",
        "suspicious-site.tk",
        "analytics.google.com"
    }
    
    actual_third_party = {d['domain'] for d in third_party_domains}
    
    if actual_third_party == expected_third_party:
        print("âœ“ ç¬¬ä¸‰æ–¹åŸŸåæå–åŠŸèƒ½æµ‹è¯•é€šè¿‡")
        return True
    else:
        print("âœ— ç¬¬ä¸‰æ–¹åŸŸåæå–åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        print(f"  æœŸæœ›: {expected_third_party}")
        print(f"  å®é™…: {actual_third_party}")
        return False


async def test_crawling_logic():
    """æµ‹è¯•å®Œæ•´çš„çˆ¬å–é€»è¾‘"""
    print("\n=== æµ‹è¯•å®Œæ•´çš„çˆ¬å–é€»è¾‘ ===")
    
    # è¿™é‡Œæˆ‘ä»¬åªåšæ¦‚å¿µéªŒè¯ï¼Œä¸å®é™…æ‰§è¡Œç½‘ç»œè¯·æ±‚
    print("æ¦‚å¿µéªŒè¯ï¼šçˆ¬å–é€»è¾‘å·²æŒ‰è¦æ±‚ä¿®æ”¹")
    print("1. ä»çˆ¬å–åˆ°çš„æ‰€æœ‰é“¾æ¥ä¸­æå–æœªå‘ç°çš„å­åŸŸå âœ“")
    print("2. å°†æ–°å‘ç°çš„å­åŸŸåè¡¥å……åˆ°å­åŸŸååˆ—è¡¨ä¸­å¹¶é‡æ–°ä»£å…¥çˆ¬å–æµç¨‹ âœ“")
    print("3. ç¬¬ä¸‰æ–¹åŸŸåç›´æ¥è®°å½•å¹¶è¿›å…¥AIåˆ†æé˜¶æ®µï¼Œä¸è¿›è¡Œæ·±åº¦çˆ¬å– âœ“")
    print("4. å…¨é‡é“¾æ¥å­˜å‚¨åŠŸèƒ½å·²å®ç° âœ“")
    
    return True


async def main():
    """ä¸»å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•çˆ¬å–é€»è¾‘...")
    
    try:
        # æµ‹è¯•å­åŸŸåæå–
        subdomain_test = await test_subdomain_extraction()
        
        # æµ‹è¯•ç¬¬ä¸‰æ–¹åŸŸåæå–
        third_party_test = await test_third_party_domain_extraction()
        
        # æµ‹è¯•å®Œæ•´çˆ¬å–é€»è¾‘
        crawling_logic = await test_crawling_logic()
        
        print("\n=== æµ‹è¯•æ€»ç»“ ===")
        print(f"å­åŸŸåæå–æµ‹è¯•: {'é€šè¿‡' if subdomain_test else 'å¤±è´¥'}")
        print(f"ç¬¬ä¸‰æ–¹åŸŸåæå–æµ‹è¯•: {'é€šè¿‡' if third_party_test else 'å¤±è´¥'}")
        print(f"çˆ¬å–é€»è¾‘æµ‹è¯•: {'é€šè¿‡' if crawling_logic else 'å¤±è´¥'}")
        
        if subdomain_test and third_party_test and crawling_logic:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼çˆ¬å–é€»è¾‘å·²æŒ‰è¦æ±‚æ­£ç¡®å®ç°ã€‚")
            return True
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°ã€‚")
            return False
            
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    result = asyncio.run(main())
    sys.exit(0 if result else 1)