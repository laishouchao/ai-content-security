#!/usr/bin/env python3
"""
æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•è„šæœ¬

æµ‹è¯•å¹¶è¡Œæ‰§è¡Œå™¨ä¸ä¼ ç»Ÿæ‰§è¡Œå™¨çš„æ€§èƒ½å¯¹æ¯”
éªŒè¯æ™ºèƒ½AIé¢„ç­›é€‰çš„æ•ˆæœ
"""

import asyncio
import time
import json
from typing import Dict, Any, List
from datetime import datetime
import uuid

from app.engines.parallel_scan_executor import ParallelScanExecutor
from app.engines.scan_executor import ScanTaskExecutor
from app.core.logging import TaskLogger


# æµ‹è¯•åŸŸååˆ—è¡¨
TEST_DOMAINS = [
    'example.com',
    'httpbin.org',
    'jsonplaceholder.typicode.com'
]

# æµ‹è¯•é…ç½®
TEST_CONFIGS = {
    'traditional': {
        'use_parallel_executor': False,
        'smart_prefilter_enabled': False,
        'dns_concurrency': 50,
        'max_subdomains': 50,
        'max_crawl_depth': 2,
        'max_pages_per_domain': 100,
        'request_delay': 1000,
        'timeout': 30
    },
    'parallel_basic': {
        'use_parallel_executor': True,
        'smart_prefilter_enabled': False,
        'dns_concurrency': 100,
        'max_subdomains': 50,
        'max_crawl_depth': 2,
        'max_pages_per_domain': 100,
        'request_delay': 500,
        'timeout': 30
    },
    'parallel_optimized': {
        'use_parallel_executor': True,
        'smart_prefilter_enabled': True,
        'dns_concurrency': 100,
        'ai_skip_threshold': 0.3,
        'max_subdomains': 50,
        'max_crawl_depth': 2,
        'max_pages_per_domain': 100,
        'request_delay': 500,
        'timeout': 30,
        'multi_viewport_capture': False,
        'enable_aggressive_caching': True
    },
    'high_performance': {
        'use_parallel_executor': True,
        'smart_prefilter_enabled': True,
        'dns_concurrency': 150,
        'ai_skip_threshold': 0.2,
        'max_subdomains': 100,
        'max_crawl_depth': 3,
        'max_pages_per_domain': 200,
        'request_delay': 200,
        'timeout': 30,
        'multi_viewport_capture': True,
        'enable_aggressive_caching': True
    }
}


class PerformanceTester:
    """æ€§èƒ½æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.results = {}
        self.test_start_time = None
        self.test_end_time = None
    
    async def run_performance_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ€§èƒ½æµ‹è¯•å¥—ä»¶"""
        print("ğŸš€ å¼€å§‹æ€§èƒ½ä¼˜åŒ–éªŒè¯æµ‹è¯•")
        self.test_start_time = time.time()
        
        # æµ‹è¯•æ‰€æœ‰é…ç½®
        for config_name, config in TEST_CONFIGS.items():
            print(f"\nğŸ“Š æµ‹è¯•é…ç½®: {config_name}")
            config_results = []
            
            for domain in TEST_DOMAINS:
                print(f"   æµ‹è¯•åŸŸå: {domain}")
                
                try:
                    result = await self._test_single_domain(domain, config, config_name)
                    config_results.append(result)
                    print(f"   âœ… å®Œæˆ - è€—æ—¶: {result['execution_time']:.2f}s")
                    
                except Exception as e:
                    print(f"   âŒ å¤±è´¥: {e}")
                    config_results.append({
                        'domain': domain,
                        'error': str(e),
                        'execution_time': 0
                    })
            
            self.results[config_name] = config_results
        
        self.test_end_time = time.time()
        
        # ç”ŸæˆæŠ¥å‘Š
        report = self._generate_performance_report()
        
        # ä¿å­˜æŠ¥å‘Š
        await self._save_test_report(report)
        
        return report
    
    async def _test_single_domain(self, domain: str, config: Dict[str, Any], config_name: str) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªåŸŸå"""
        task_id = str(uuid.uuid4())
        user_id = "test_user"
        
        start_time = time.time()
        
        # é€‰æ‹©æ‰§è¡Œå™¨
        if config.get('use_parallel_executor', False):
            executor = ParallelScanExecutor(task_id, user_id)
            result = await executor.execute_scan(domain, config)
            
            # æå–æ€§èƒ½æŒ‡æ ‡
            results_data = result.get('results', {})
            ai_calls_made = 0
            ai_calls_skipped = 0
            
            # ä»äº‹ä»¶ä¸­ç»Ÿè®¡AIè°ƒç”¨
            events = result.get('events', [])
            for event in events:
                if event.get('event_type') == 'ai_analysis_skipped':
                    ai_calls_skipped += 1
                elif event.get('event_type') == 'domain_analyzed':
                    ai_calls_made += 1
            
        else:
            # ä¼ ç»Ÿæ‰§è¡Œå™¨ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
            executor = ScanTaskExecutor(task_id, user_id)
            result = await executor.execute_scan(domain, config)
            
            results_data = {
                'subdomains': getattr(result, 'subdomains', []),
                'third_party_domains': getattr(result, 'third_party_domains', []),
                'statistics': getattr(result, 'statistics', {})
            }
            ai_calls_made = results_data['statistics'].get('ai_analysis_completed', 0)
            ai_calls_skipped = 0
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # è¿”å›æ€§èƒ½æŒ‡æ ‡
        return {
            'domain': domain,
            'config_name': config_name,
            'execution_time': execution_time,
            'subdomains_found': len(results_data.get('subdomains', [])),
            'third_party_domains': len(results_data.get('third_party_domains', [])),
            'ai_calls_made': ai_calls_made,
            'ai_calls_skipped': ai_calls_skipped,
            'ai_skip_rate': (ai_calls_skipped / (ai_calls_made + ai_calls_skipped) * 100) if (ai_calls_made + ai_calls_skipped) > 0 else 0,
            'executor_type': 'parallel' if config.get('use_parallel_executor') else 'traditional',
            'smart_prefilter_enabled': config.get('smart_prefilter_enabled', False)
        }
    
    def _generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        report = {
            'test_summary': {
                'start_time': datetime.fromtimestamp(self.test_start_time or time.time()).isoformat(),
                'end_time': datetime.fromtimestamp(self.test_end_time or time.time()).isoformat(),
                'total_duration': (self.test_end_time or 0) - (self.test_start_time or 0),
                'configs_tested': len(TEST_CONFIGS),
                'domains_tested': len(TEST_DOMAINS)
            },
            'detailed_results': self.results,
            'performance_comparison': self._calculate_performance_comparison(),
            'recommendations': self._generate_recommendations()
        }
        
        return report
    
    def _calculate_performance_comparison(self) -> Dict[str, Any]:
        """è®¡ç®—æ€§èƒ½å¯¹æ¯”"""
        comparison = {}
        
        # è®¡ç®—æ¯ä¸ªé…ç½®çš„å¹³å‡æ€§èƒ½
        for config_name, results in self.results.items():
            if not results:
                continue
            
            valid_results = [r for r in results if 'error' not in r]
            if not valid_results:
                continue
            
            avg_execution_time = sum(r['execution_time'] for r in valid_results) / len(valid_results)
            total_subdomains = sum(r['subdomains_found'] for r in valid_results)
            total_ai_calls = sum(r['ai_calls_made'] for r in valid_results)
            total_ai_skipped = sum(r['ai_calls_skipped'] for r in valid_results)
            
            comparison[config_name] = {
                'avg_execution_time': avg_execution_time,
                'total_subdomains': total_subdomains,
                'total_ai_calls': total_ai_calls,
                'total_ai_skipped': total_ai_skipped,
                'ai_skip_rate': (total_ai_skipped / (total_ai_calls + total_ai_skipped) * 100) if (total_ai_calls + total_ai_skipped) > 0 else 0,
                'efficiency_score': total_subdomains / avg_execution_time if avg_execution_time > 0 else 0
            }
        
        # è®¡ç®—æ€§èƒ½æå‡å€æ•°
        if 'traditional' in comparison and 'parallel_optimized' in comparison:
            traditional_time = comparison['traditional']['avg_execution_time']
            optimized_time = comparison['parallel_optimized']['avg_execution_time']
            
            if optimized_time > 0:
                speedup = traditional_time / optimized_time
                comparison['performance_improvement'] = {
                    'speedup_factor': speedup,
                    'time_reduction_percentage': ((traditional_time - optimized_time) / traditional_time) * 100,
                    'ai_cost_reduction': comparison['parallel_optimized']['ai_skip_rate']
                }
        
        return comparison
    
    def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        comparison = self._calculate_performance_comparison()
        
        if 'performance_improvement' in comparison:
            improvement = comparison['performance_improvement']
            
            if improvement['speedup_factor'] > 2:
                recommendations.append(f"ğŸš€ å¹¶è¡Œæ‰§è¡Œå™¨æ˜¾è‘—æå‡æ€§èƒ½ {improvement['speedup_factor']:.1f}xï¼Œå¼ºçƒˆæ¨èå¯ç”¨")
            
            if improvement['ai_cost_reduction'] > 50:
                recommendations.append(f"ğŸ’° æ™ºèƒ½AIé¢„ç­›é€‰èŠ‚çœ {improvement['ai_cost_reduction']:.1f}% æˆæœ¬ï¼Œå»ºè®®å¯ç”¨")
        
        # æ ¹æ®æµ‹è¯•ç»“æœç”Ÿæˆå…·ä½“å»ºè®®
        if 'high_performance' in comparison:
            hp_score = comparison['high_performance']['efficiency_score']
            recommendations.append(f"âš¡ é«˜æ€§èƒ½é…ç½®æ•ˆç‡è¯„åˆ†: {hp_score:.2f}ï¼Œé€‚åˆå¤§è§„æ¨¡æ‰«æ")
        
        recommendations.extend([
            "ğŸ“Š å»ºè®®æ ¹æ®å…·ä½“ä¸šåŠ¡éœ€æ±‚é€‰æ‹©é…ç½®é¢„è®¾",
            "ğŸ”§ å¯æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´DNSå¹¶å‘æ•°",
            "ğŸ¯ å®šæœŸç›‘æ§AIè·³è¿‡ç‡ä»¥å¹³è¡¡æˆæœ¬å’Œå‡†ç¡®æ€§"
        ])
        
        return recommendations
    
    async def _save_test_report(self, report: Dict[str, Any]):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"performance_test_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False, default=str)
            
            print(f"\nğŸ“‹ æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜: {filename}")
            
            # æ‰“å°æ‘˜è¦
            self._print_test_summary(report)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")
    
    def _print_test_summary(self, report: Dict[str, Any]):
        """æ‰“å°æµ‹è¯•æ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ¯ æ€§èƒ½ä¼˜åŒ–æµ‹è¯•æ‘˜è¦")
        print("="*60)
        
        summary = report['test_summary']
        print(f"ğŸ“… æµ‹è¯•æ—¶é—´: {summary['start_time']} ~ {summary['end_time']}")
        print(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
        print(f"ğŸ§ª æµ‹è¯•é…ç½®: {summary['configs_tested']}ä¸ª")
        print(f"ğŸŒ æµ‹è¯•åŸŸå: {summary['domains_tested']}ä¸ª")
        
        comparison = report['performance_comparison']
        
        if 'performance_improvement' in comparison:
            improvement = comparison['performance_improvement']
            print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
            print(f"   ğŸš€ é€Ÿåº¦æå‡: {improvement['speedup_factor']:.1f}x")
            print(f"   â±ï¸  æ—¶é—´å‡å°‘: {improvement['time_reduction_percentage']:.1f}%")
            print(f"   ğŸ’° AIæˆæœ¬èŠ‚çœ: {improvement['ai_cost_reduction']:.1f}%")
        
        print(f"\nğŸ’¡ ä¼˜åŒ–å»ºè®®:")
        for rec in report['recommendations']:
            print(f"   {rec}")
        
        print("\n" + "="*60)


async def main():
    """ä¸»å‡½æ•°"""
    tester = PerformanceTester()
    
    try:
        report = await tester.run_performance_tests()
        print("\nâœ… æ€§èƒ½æµ‹è¯•å®Œæˆ!")
        return report
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())